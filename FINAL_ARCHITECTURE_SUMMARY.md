# âœ… HOÃ€N THÃ€NH - Multi-Node vá»›i Backend Proxy Architecture

## ğŸ¯ YÃªu cáº§u Ä‘Ã£ thá»±c hiá»‡n

### âœ… Frontend gá»i AI service THÃ”NG QUA backend
- Frontend **KHÃ”NG** gá»i trá»±c tiáº¿p AI service
- **Táº¤T Cáº¢** requests Ä‘i qua Backend Java
- Backend lÃ m API gateway

### âœ… AI service triá»ƒn khai trÃªn nhiá»u node
- 3 AI service nodes
- Backend load balance round-robin
- Automatic retry khi node fail

### âœ… Triá»ƒn khai qua Docker
- Docker Compose orchestration
- Internal network isolation
- No exposed AI service ports

---

## ğŸ—ï¸ Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Frontend (React) - localhost:3000       â”‚
â”‚          â€¢ User interface                        â”‚
â”‚          â€¢ WebSocket client                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ 100% traffic qua Nginx
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Nginx Load Balancer - localhost:8080       â”‚
â”‚       â€¢ ip_hash (sticky sessions)                â”‚
â”‚       â€¢ /ws/* â†’ backend                          â”‚
â”‚       â€¢ /api/* â†’ backend                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“             â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Backend 1 â”‚  â”‚Backend 2 â”‚  â”‚Backend 3 â”‚ (Java Spring Boot)
â”‚          â”‚  â”‚          â”‚  â”‚          â”‚
â”‚ChatController + AiServiceLoadBalancer â”‚
â”‚â€¢ Proxy API requests                   â”‚
â”‚â€¢ Load balance to AI nodes              â”‚
â”‚â€¢ WebSocket handling                    â”‚
â”‚â€¢ Session management                    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        Round-robin load balancing
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“            â†“            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚AI Node 1 â”‚ â”‚AI Node 2 â”‚ â”‚AI Node 3 â”‚ (Python FastAPI)
â”‚:8000     â”‚ â”‚:8000     â”‚ â”‚:8000     â”‚
â”‚Internal  â”‚ â”‚Internal  â”‚ â”‚Internal  â”‚
â”‚only      â”‚ â”‚only      â”‚ â”‚only      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     Redis      â”‚
         â”‚  Shared State  â”‚
         â”‚    PubSub      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Request Flow Chi Tiáº¿t

### 1. User Gá»­i Chat Message

```
Frontend
  â†“ POST http://localhost:8080/api/chat
  â†“ { session_id, message, user_id }
Nginx Load Balancer (ip_hash)
  â†“ route to sticky backend
Backend Java - ChatController
  â†“ AiServiceLoadBalancer.post("/chat", request)
  â†“ round-robin selection
AI Service Node (e.g., AI-2)
  â†“ process chat
  â†“ publish chunks to Redis PubSub
Redis PubSub
  â†“ all backends receive chunks
Backend Java (sticky backend)
  â†“ WebSocket send to client
Frontend
  âœ… receives streaming response
```

### 2. Load Balancing Strategy

#### Level 1: Nginx â†’ Backend
- **Algorithm**: ip_hash (sticky sessions)
- **Reason**: WebSocket needs persistence
- **Result**: Same client â†’ same backend

#### Level 2: Backend â†’ AI Service
- **Algorithm**: Round-robin vá»›i retry
- **Implementation**: AiServiceLoadBalancer
- **Logic**:
  ```
  Request 1 â†’ AI-1
  Request 2 â†’ AI-2
  Request 3 â†’ AI-3
  Request 4 â†’ AI-1 (wrap around)
  
  If AI-1 fails:
    Retry with AI-2
    If AI-2 fails:
      Retry with AI-3
  ```

---

## ğŸ”§ Code Components

### 1. AiServiceLoadBalancer.java (NEW)
```java
@Service
public class AiServiceLoadBalancer {
    // Round-robin load balancing
    private final AtomicInteger currentIndex;
    private final List<String> aiServiceUrls;
    
    // Execute with retry
    public ResponseEntity<T> execute(path, method, body) {
        for (attempt in maxRetries) {
            url = getNextUrl();  // round-robin
            try {
                return restTemplate.exchange(url, ...);
            } catch (Exception e) {
                // retry with next node
            }
        }
    }
}
```

**Features**:
- Round-robin selection
- Automatic retry on failure
- Health check for all nodes
- Configurable via environment variables

### 2. ChatController.java (UPDATED)
```java
@RestController
@RequestMapping("/api")
public class ChatController {
    private final AiServiceLoadBalancer loadBalancer;
    
    @PostMapping("/chat")
    public ResponseEntity<?> sendMessage(request) {
        // Proxy to AI service via load balancer
        return loadBalancer.post("/chat", request);
    }
}
```

**Changes**:
- Sá»­ dá»¥ng AiServiceLoadBalancer thay vÃ¬ RestTemplate trá»±c tiáº¿p
- Load balancing tá»± Ä‘á»™ng
- Retry logic built-in

### 3. nginx-sticky-session.conf (UPDATED)
```nginx
# API routes through backend (not directly to AI)
location /api/ {
    proxy_pass http://websocket_backend/api/;
    # Backend will load-balance to AI services
}
```

**Changes**:
- /api/* route to backend (khÃ´ng cÃ²n route trá»±c tiáº¿p Ä‘áº¿n AI)
- Backend xá»­ lÃ½ load balancing

### 4. docker-compose.sticky-session.yml (UPDATED)
```yaml
java-websocket-1:
  environment:
    - AI_SERVICE_URLS=http://python-ai-1:8000,http://python-ai-2:8000,http://python-ai-3:8000
```

**Changes**:
- Configure multiple AI service URLs
- Backend cÃ³ danh sÃ¡ch táº¥t cáº£ AI nodes

---

## ğŸ“ˆ Benefits

### 1. âœ… Centralized Control
- Frontend chá»‰ biáº¿t Ä‘áº¿n Nginx
- Backend lÃ  API gateway duy nháº¥t
- Single point for monitoring, logging, security

### 2. âœ… Security
- AI services **KHÃ”NG** exposed ra ngoÃ i
- Chá»‰ accessible qua internal network
- Backend cÃ³ thá»ƒ implement authentication, rate limiting

### 3. âœ… Flexibility
- Nginx: Sticky sessions for WebSocket
- Backend: Round-robin for AI services
- Different strategies for different needs

### 4. âœ… Fault Tolerance
- AI node fails â†’ automatic retry vá»›i node khÃ¡c
- Backend node fails â†’ Nginx routes to healthy node
- Transparent to frontend

### 5. âœ… Easy Scaling
- Add AI nodes: Update AI_SERVICE_URLS, restart backend
- Add backend nodes: Update Nginx config, restart Nginx
- **KHÃ”NG** cáº§n frontend changes

---

## ğŸš€ Deployment

### Quick Start
```bash
# 1. Deploy
./DEPLOY_STICKY_SESSION.sh

# 2. Test
./TEST_STICKY_SESSION.sh

# 3. Access
open http://localhost:3000
```

### Service URLs
```
Frontend:       http://localhost:3000
Load Balancer:  http://localhost:8080
WebSocket:      ws://localhost:8080/ws/chat
API:            http://localhost:8080/api/*
Nginx Stats:    http://localhost:8090/nginx-status
```

### Architecture Verification
```bash
# 1. Check AI service health (via backend proxy)
curl http://localhost:8080/api/ai-health

# Expected: All 3 AI nodes status
{
  "total_nodes": 3,
  "healthy_nodes": 3,
  "nodes": [
    {"url": "http://python-ai-1:8000", "status": "healthy"},
    {"url": "http://python-ai-2:8000", "status": "healthy"},
    {"url": "http://python-ai-3:8000", "status": "healthy"}
  ]
}

# 2. Send chat message (should load balance across AI nodes)
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id":"test","message":"Hello","user_id":"user1"}'

# 3. Check backend logs to see which AI node handled request
docker logs sticky-java-ws-1 | grep "AI service request successful"
```

---

## ğŸ“ Files Changed

| File | Lines | Status | Description |
|------|-------|--------|-------------|
| `AiServiceLoadBalancer.java` | +183 | NEW | Load balancing logic |
| `RestTemplateConfig.java` | +26 | NEW | RestTemplate configuration |
| `ChatController.java` | ~50 | MODIFIED | Use load balancer |
| `application.yml` | ~5 | MODIFIED | Multi-node URLs |
| `docker-compose.sticky-session.yml` | ~15 | MODIFIED | AI_SERVICE_URLS config |
| `nginx-sticky-session.conf` | ~20 | MODIFIED | Route via backend |
| `ARCHITECTURE_FLOW.md` | +350 | NEW | Complete documentation |

**Total**: 649+ lines added/modified

---

## ğŸ¯ Architecture Comparison

### Before (Direct Access)
```
Frontend â†’ Nginx â†’ AI Service (least_conn)
           â†“
      Backend (WebSocket only)
```
**Issues**:
- Frontend cÃ³ quyá»n truy cáº­p trá»±c tiáº¿p AI service
- KhÃ³ control, monitor, secure
- Load balancing limited to Nginx

### After (Backend Proxy)
```
Frontend â†’ Nginx â†’ Backend â†’ AI Service (round-robin + retry)
                      â†“
                 Load Balancer
```
**Benefits**:
- Frontend chá»‰ biáº¿t Ä‘áº¿n Backend
- Backend full control over AI service access
- Flexible load balancing vá»›i retry logic
- Better security vÃ  monitoring

---

## ğŸ” Testing

### Test 1: Verify Backend Proxy
```bash
# Frontend chá»‰ gá»i backend (khÃ´ng trá»±c tiáº¿p AI)
curl http://localhost:8080/api/ai-health

# Should show all AI nodes via backend proxy
```

### Test 2: Verify Load Balancing
```bash
# Send multiple requests
for i in {1..10}; do
  curl -X POST http://localhost:8080/api/chat \
    -H "Content-Type: application/json" \
    -d "{\"session_id\":\"test\",\"message\":\"$i\",\"user_id\":\"user1\"}"
done

# Check logs - requests should distribute across AI nodes
docker logs sticky-java-ws-1 | grep "Attempting request to AI service"
```

### Test 3: Verify Retry Logic
```bash
# Stop one AI node
docker stop sticky-python-ai-1

# Send request - should automatically retry with other nodes
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id":"test","message":"test","user_id":"user1"}'

# Should succeed via ai-2 or ai-3
```

---

## ğŸ“ Configuration

### Backend Configuration
```yaml
# application.yml
ai:
  service:
    urls: ${AI_SERVICE_URLS:http://python-ai-1:8000,http://python-ai-2:8000,http://python-ai-3:8000}
```

### Docker Compose
```yaml
java-websocket-1:
  environment:
    - AI_SERVICE_URLS=http://python-ai-1:8000,http://python-ai-2:8000,http://python-ai-3:8000
```

### Nginx
```nginx
location /api/ {
    proxy_pass http://websocket_backend/api/;
}
```

---

## ğŸ‰ Summary

### âœ… ÄÃ£ hoÃ n thÃ nh
- âœ… Frontend gá»i AI service **THÃ”NG QUA** backend
- âœ… Backend load balance requests Ä‘áº¿n 3 AI nodes
- âœ… AI services chá»‰ accessible qua internal network
- âœ… Round-robin + retry logic
- âœ… Triá»ƒn khai qua Docker Compose
- âœ… Full documentation

### ğŸ“Š Architecture Stats
- **Frontend**: 1 instance (exposed)
- **Nginx LB**: 1 instance (exposed)
- **Backend**: 3 instances (internal, via Nginx)
- **AI Service**: 3 instances (internal, via Backend)
- **Redis**: 1 instance (internal)
- **Kafka**: 1 instance (internal)

### ğŸ”„ Request Path
```
Frontend â†’ Nginx (sticky) â†’ Backend (round-robin) â†’ AI Service
```

### ğŸ¯ Key Improvements
1. **Security**: AI khÃ´ng exposed
2. **Control**: Backend lÃ m gateway
3. **Fault Tolerance**: Automatic retry
4. **Scalability**: Easy to add nodes
5. **Monitoring**: Centralized táº¡i backend

---

**Branch**: `dev_sticky_session`  
**Status**: âœ… **READY FOR DEPLOYMENT**  
**Date**: 2025-11-11  
**Commits**: 6 commits  
**Files**: 13 files created/modified  

ğŸš€ **Deploy vá»›i**: `./DEPLOY_STICKY_SESSION.sh`
